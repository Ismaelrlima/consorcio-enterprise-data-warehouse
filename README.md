# ConsÃ³rcio Data Warehouse Pipeline

Pipeline de Engenharia de Dados desenvolvido em Python com arquitetura
em camadas (Bronze â†’ Silver â†’ Gold), modelagem dimensional em Star
Schema e carga estruturada em PostgreSQL.

Este projeto demonstra aplicaÃ§Ã£o prÃ¡tica de princÃ­pios fundamentais de
Data Engineering: ingestÃ£o estruturada, transformaÃ§Ã£o semÃ¢ntica,
modelagem dimensional, idempotÃªncia, separaÃ§Ã£o de responsabilidades e
execuÃ§Ã£o containerizada.

------------------------------------------------------------------------

## VisÃ£o Geral

O pipeline realiza:

-   ExtraÃ§Ã£o de dados via API
-   PadronizaÃ§Ã£o e tratamento de dados
-   GeraÃ§Ã£o de hash para controle de duplicidade
-   ConstruÃ§Ã£o de dimensÃµes com surrogate keys
-   CriaÃ§Ã£o de tabela fato
-   Carga estruturada em Data Warehouse PostgreSQL

A arquitetura foi projetada com foco em organizaÃ§Ã£o, reprocessamento
seguro e evoluÃ§Ã£o para cenÃ¡rios produtivos.

------------------------------------------------------------------------

## Arquitetura

O projeto adota separaÃ§Ã£o clara de responsabilidades seguindo o padrÃ£o:

Bronze â†’ Silver â†’ Gold

### Bronze (IngestÃ£o)

-   Consumo da API
-   ConversÃ£o para DataFrame
-   PreservaÃ§Ã£o do dado bruto

### Silver (Tratamento)

-   PadronizaÃ§Ã£o de colunas (snake_case)
-   ConversÃ£o de tipos
-   Tratamento de datas
-   NormalizaÃ§Ã£o de valores numÃ©ricos
-   GeraÃ§Ã£o de `row_hash`
-   PreparaÃ§Ã£o para modelagem dimensional

### Gold (Modelagem Dimensional)

-   ConstruÃ§Ã£o de dimensÃµes
-   GeraÃ§Ã£o de surrogate keys (SK)
-   Montagem da tabela fato
-   Relacionamentos via chaves substitutas

------------------------------------------------------------------------

## Modelo Dimensional

### DimensÃµes

dim_vendedor\
- sk_vendedor (PK)\
- id_pessoa_vendedor (natural key)\
- vendedor

dim_administradora\
- sk_administradora (PK)\
- id_administradora (natural key)\
- administradora

dim_tempo\
- sk_tempo (PK)\
- data_venda\
- ano\
- mes\
- dia\
- dia_da_semana\
- trimestre

### Fato

fato_vendas\
- sk_vendedor (FK)\
- sk_administradora (FK)\
- sk_tempo (FK)\
- valor_credito\
- valor_primeira_parcela\
- row_hash

A tabela fato mantÃ©m exclusivamente mÃ©tricas e chaves substitutas,
seguindo boas prÃ¡ticas de modelagem dimensional.

------------------------------------------------------------------------

## IdempotÃªncia e Controle de Duplicidade

O pipeline gera um `row_hash` com base nos atributos relevantes da
venda.

Isso permite:

-   ReexecuÃ§Ã£o segura
-   Evitar duplicidade de registros
-   Garantir consistÃªncia em cargas repetidas
-   Base estrutural para futura carga incremental

------------------------------------------------------------------------

## Logging e Observabilidade

O projeto utiliza logging estruturado para:

-   Monitoramento da execuÃ§Ã£o
-   Rastreabilidade de falhas
-   Auditoria de cargas

Os logs sÃ£o persistidos em arquivo (`pipeline.log`) e podem ser
integrados a soluÃ§Ãµes externas de observabilidade.

------------------------------------------------------------------------

## Estrutura do Projeto

ğŸ“¦ src\
â”£ ğŸ“‚ bronze\
â”ƒ â”— ğŸ“œ bronze_layer.py\
â”£ ğŸ“‚ gold\
â”ƒ â”£ ğŸ“œ dimensions.py\
â”ƒ â”£ ğŸ“œ fact.py\
â”ƒ â”£ ğŸ“œ gold_pipeline.py\
â”ƒ â”— ğŸ“œ incremental.py\
â”£ ğŸ“‚ ingestion\
â”ƒ â”— ğŸ“œ microwork_api.py\
â”£ ğŸ“‚ load\
â”ƒ â”— ğŸ“œ dev_load_gold.py\
â”£ ğŸ“‚ logs\
â”ƒ â”— ğŸ“œ pipeline.log\
â”£ ğŸ“‚ silver\
â”ƒ â”— ğŸ“œ silver_layer.py\
â”£ ğŸ“‚ sql\
â”ƒ â”— ğŸ“œ create_dw.sql\
â”£ ğŸ“‚ utils\
â”ƒ â”£ ğŸ“œ extrator.py\
â”ƒ â”— ğŸ“œ logging_config.py\
â”£ ğŸ“œ main.py\
â”— ğŸ“œ requirements.txt

------------------------------------------------------------------------

## Tecnologias Utilizadas

-   Python 3.10+
-   Pandas
-   SQLAlchemy
-   PostgreSQL
-   Docker
-   Logging padrÃ£o da biblioteca Python

------------------------------------------------------------------------

## ExecuÃ§Ã£o (Ambiente Containerizado)

### 1. Clonar o repositÃ³rio

git clone https://github.com/Ismaelrlima/consorcio-enterprise-data-warehouse.git
cd consorcio-pipeline

### 2. Criar arquivo .env

Criar um arquivo `.env` na raiz do projeto:

DB_HOST=localhost\
DB_PORT=5432\
DB_USER=usuario\
DB_PASSWORD=senha\
DB_NAME=banco

### 3. Build da imagem Docker

docker build -t consorcio-pipeline .

### 4. Executar container

docker run --env-file .env consorcio-pipeline

A execuÃ§Ã£o do pipeline ocorre automaticamente via `main.py` dentro do
container.

------------------------------------------------------------------------

## DecisÃµes TÃ©cnicas

-   Arquitetura em camadas para escalabilidade e manutenÃ§Ã£o
-   Uso de surrogate keys para desacoplamento de chaves naturais
-   Star Schema para otimizaÃ§Ã£o analÃ­tica
-   Hashing para garantir idempotÃªncia
-   SeparaÃ§Ã£o clara entre ingestÃ£o, transformaÃ§Ã£o e carga
-   ContainerizaÃ§Ã£o para reprodutibilidade de ambiente

------------------------------------------------------------------------

## ConsideraÃ§Ãµes de Performance

-   Uso de operaÃ§Ãµes vetorizadas com Pandas
-   OrganizaÃ§Ã£o em camadas para evitar reprocessamento desnecessÃ¡rio
-   Estrutura preparada para evoluÃ§Ã£o para processamento incremental

------------------------------------------------------------------------

## PrÃ³ximos Passos TÃ©cnicos

-   ImplementaÃ§Ã£o formal de carga incremental
-   SCD (Slowly Changing Dimensions)
-   Ãndices e constraints automÃ¡ticas no DW
-   Docker Compose para orquestraÃ§Ã£o com PostgreSQL
-   OrquestraÃ§Ã£o com Airflow
-   Testes automatizados

------------------------------------------------------------------------

## Conceitos Demonstrados

-   Data Warehousing
-   Modelagem Dimensional
-   Star Schema
-   Surrogate Keys
-   IdempotÃªncia em pipelines
-   Arquitetura modular
-   SeparaÃ§Ã£o de responsabilidades
-   ContainerizaÃ§Ã£o de aplicaÃ§Ãµes

------------------------------------------------------------------------

Projeto desenvolvido como prÃ¡tica aplicada de Engenharia de Dados com
foco em organizaÃ§Ã£o arquitetural, modelagem analÃ­tica e boas prÃ¡ticas
alinhadas ao mercado.
